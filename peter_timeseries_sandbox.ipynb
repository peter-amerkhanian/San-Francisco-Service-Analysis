{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import retrieval_311\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, Lasso\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, SimpleRNN\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/processed_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>datetime</th>\n",
       "      <th>calls</th>\n",
       "      <th>month</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>dayofmonth</th>\n",
       "      <th>hourofday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2008-07-01 00:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2008-07-01 01:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2008-07-01 02:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2008-07-01 03:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2008-07-01 04:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0             datetime  calls  month  dayofweek  dayofmonth  \\\n",
       "0           0  2008-07-01 00:00:00      2      7          1           1   \n",
       "1           1  2008-07-01 01:00:00      0      7          1           1   \n",
       "2           2  2008-07-01 02:00:00      0      7          1           1   \n",
       "3           3  2008-07-01 03:00:00      1      7          1           1   \n",
       "4           4  2008-07-01 04:00:00      0      7          1           1   \n",
       "\n",
       "   hourofday  \n",
       "0          0  \n",
       "1          1  \n",
       "2          2  \n",
       "3          3  \n",
       "4          4  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['datetime'] = pd.to_datetime(df['datetime'],format=\"%Y-%m-%d\")\n",
    "df = df[(df['datetime'].dt.year > 2008) & (df['datetime'].dt.year < 2023)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"Unnamed: 0\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    122712.000000\n",
       "mean         17.626622\n",
       "std          22.591759\n",
       "min           0.000000\n",
       "25%           2.000000\n",
       "50%           8.000000\n",
       "75%          25.000000\n",
       "max         553.000000\n",
       "Name: calls, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['calls'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2162998"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['calls'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year'] = df.index.year\n",
    "df['date'] = df.index.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "year_group = df.groupby(\"year\")['calls'].sum()\n",
    "year_group = year_group.iloc[:-1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "ax.plot(year_group, \".-\", linewidth=4, markersize=15)\n",
    "ax.set_xticks(year_group.index)\n",
    "ax.set_xticklabels([\"'\"+str(y)[-2:] for y in year_group.index])\n",
    "ax.grid(alpha=.4)\n",
    "fmt = '{x:,.0f}'\n",
    "tick = mtick.StrMethodFormatter(fmt)\n",
    "ax.yaxis.set_major_formatter(tick)\n",
    "ax.set(title=\"Street and Sidewalk Cleaning Calls\", xlabel=\"Year\", ylabel=\"Annual Calls\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"figures/annual_trend.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fit(actual_calls, test_preds):\n",
    "    rmse = np.mean((actual_calls - test_preds)**2)**.5\n",
    "    r2 = r2_score(actual_calls, test_preds)\n",
    "    max_resid = np.max(np.abs(actual_calls - test_preds))\n",
    "    print(\"rmse\", rmse)\n",
    "    print(\"r2\", r2)\n",
    "    print(\"max resid\", max_resid)\n",
    "    return rmse, r2, max_resid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Naive Baseline Model -----\n",
      "2009 --> 2010\n",
      "rmse 5.609386810131294\n",
      "r2 0.2004180223499794\n",
      "max resid 38.0\n",
      "2010 --> 2011\n",
      "rmse 6.305552015953506\n",
      "r2 0.24044542701890992\n",
      "max resid 41.0\n",
      "2011 --> 2012\n",
      "rmse 6.551265307937561\n",
      "r2 0.18907831378440365\n",
      "max resid 117.0\n",
      "2012 --> 2013\n",
      "rmse 7.450316203924605\n",
      "r2 0.24800447678655924\n",
      "max resid 133.0\n",
      "2013 --> 2014\n",
      "rmse 8.926647306776358\n",
      "r2 0.3461654111005241\n",
      "max resid 152.0\n",
      "2014 --> 2015\n",
      "rmse 10.190725946506678\n",
      "r2 0.3378691038852478\n",
      "max resid 142.0\n",
      "2015 --> 2016\n",
      "rmse 14.566150837800919\n",
      "r2 0.35390855235440943\n",
      "max resid 105.0\n",
      "2016 --> 2017\n",
      "rmse 15.55052029093179\n",
      "r2 0.4531249436179995\n",
      "max resid 91.0\n",
      "2017 --> 2018\n",
      "rmse 17.036472380882522\n",
      "r2 0.5011373804374825\n",
      "max resid 239.0\n",
      "2018 --> 2019\n",
      "rmse 18.018043960780965\n",
      "r2 0.5542244910779695\n",
      "max resid 263.0\n",
      "2019 --> 2020\n",
      "rmse 19.441971179448757\n",
      "r2 0.5620363490435625\n",
      "max resid 289.0\n",
      "2020 --> 2021\n",
      "rmse 21.3977838964145\n",
      "r2 0.5667698208998744\n",
      "max resid 442.0\n",
      "\n",
      "Mean RMSE: 12.587069678124122\n",
      "Mean r2: 0.37943185769641014\n",
      "Max resid: 171.0\n"
     ]
    }
   ],
   "source": [
    "all_years = df.index.year.unique()\n",
    "\n",
    "print(\"----- Naive Baseline Model -----\")\n",
    "rmses = []\n",
    "r2s = []\n",
    "max_resids = []\n",
    "for i in range(len(all_years)-2):\n",
    "    print(all_years[i], \"-->\", all_years[i+1])\n",
    "    train_data = df[df.index.year == all_years[i]]\n",
    "    test_data = df[df.index.year == all_years[i+1]]\n",
    "    lag = 24*2\n",
    "    test_preds = train_data['calls'].shift(lag).dropna()\n",
    "    test_preds.index = test_preds.index + pd.DateOffset(years=1)\n",
    "    actual_calls = test_data['calls'].iloc[lag:]\n",
    "    merged = pd.merge(test_preds, actual_calls, left_index=True, right_index=True)\n",
    "    actual_calls = merged['calls_y']\n",
    "    test_preds = merged['calls_x']\n",
    "    rmse, r2, max_resid = evaluate_fit(actual_calls, test_preds)\n",
    "    rmses.append(rmse)\n",
    "    r2s.append(r2)\n",
    "    max_resids.append(max_resid)\n",
    "print()\n",
    "print(\"Mean RMSE:\", np.mean(rmses))\n",
    "print(\"Mean r2:\", np.mean(r2s))\n",
    "print(\"Max resid:\", np.mean(max_resids))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_years = df.index.year.unique()\n",
    "\n",
    "# rmses = []\n",
    "# r2s = []\n",
    "# max_resids = []\n",
    "# for i in range(len(all_years)-2):\n",
    "#     print(all_years[i], \"-->\", all_years[i+1])\n",
    "#     train_data = df[df.index.year == all_years[i]]\n",
    "#     test_data = df[df.index.year == all_years[i+1]]\n",
    "#     preds = (train_data\n",
    "#              .groupby(['date', 'hourofday'])\n",
    "#              ['calls']\n",
    "#              .mean()).to_dict()\n",
    "#     test_preds = (\n",
    "#         test_data\n",
    "#         .apply(lambda x: preds.get(((x.date - pd.DateOffset(years=1)).to_pydatetime().date(),\n",
    "#                                     x.hourofday)),\n",
    "#                 axis=1)\n",
    "#     )\n",
    "#     print(test_preds.isna().sum())\n",
    "#     rmse = np.mean((test_data['calls'] - test_preds)**2)**.5\n",
    "#     r2 = r2_score(test_data['calls'], test_preds, )\n",
    "#     max_resid = np.max(np.abs(test_data['calls'] - test_preds))\n",
    "#     print(\"rmse\", rmse)\n",
    "#     print(\"r2\", r2)\n",
    "#     print(\"max resid\", max_resid)\n",
    "#     rmses.append(rmse)\n",
    "#     r2s.append(r2)\n",
    "#     max_resids.append(max_resid)\n",
    "# print(\"Mean RMSE:\", np.mean(rmses))\n",
    "# print(\"Mean r2:\", np.mean(r2s))\n",
    "# print(\"Max resid:\", np.mean(max_resids))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_years = df.index.year.unique()\n",
    "\n",
    "# rmses = []\n",
    "# for i in range(len(all_years)-2):\n",
    "#     print(all_years[i], \"-->\", all_years[i+1])\n",
    "#     train_data = df[df.index.year == all_years[i]]\n",
    "#     test_data = df[df.index.year == all_years[i+1]]\n",
    "#     preds = (train_data\n",
    "#              .groupby([train_data['month'],\n",
    "#                        train_data['dayofmonth'],\n",
    "#                        train_data['hourofday']])\n",
    "#              ['calls']\n",
    "#              .max()).to_dict()\n",
    "#     test_preds = (\n",
    "#         test_data\n",
    "#         .apply(lambda x: preds.get((x.month,\n",
    "#                                     x.dayofmonth,\n",
    "#                                     x.hourofday)),\n",
    "#                 axis=1)\n",
    "#     )\n",
    "#     rmse = np.mean((test_data['calls'] - test_preds)**2)**.5\n",
    "#     print(rmse)\n",
    "#     rmses.append(rmse)\n",
    "# print(\"Mean RMSE:\", np.mean(rmses))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 48\n",
    "X_ = df.copy()[['calls']]\n",
    "for i in range(1, n_steps+1):\n",
    "    X_[f'lag_{i}'] = X_['calls'].shift(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = X_.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>calls</th>\n",
       "      <th>lag_1</th>\n",
       "      <th>lag_2</th>\n",
       "      <th>lag_3</th>\n",
       "      <th>lag_4</th>\n",
       "      <th>lag_5</th>\n",
       "      <th>lag_6</th>\n",
       "      <th>lag_7</th>\n",
       "      <th>lag_8</th>\n",
       "      <th>lag_9</th>\n",
       "      <th>...</th>\n",
       "      <th>lag_39</th>\n",
       "      <th>lag_40</th>\n",
       "      <th>lag_41</th>\n",
       "      <th>lag_42</th>\n",
       "      <th>lag_43</th>\n",
       "      <th>lag_44</th>\n",
       "      <th>lag_45</th>\n",
       "      <th>lag_46</th>\n",
       "      <th>lag_47</th>\n",
       "      <th>lag_48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>calls</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.848958</td>\n",
       "      <td>0.714361</td>\n",
       "      <td>0.577899</td>\n",
       "      <td>0.447423</td>\n",
       "      <td>0.325441</td>\n",
       "      <td>0.210614</td>\n",
       "      <td>0.103827</td>\n",
       "      <td>0.001355</td>\n",
       "      <td>-0.091820</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.105373</td>\n",
       "      <td>-0.014764</td>\n",
       "      <td>0.084546</td>\n",
       "      <td>0.188583</td>\n",
       "      <td>0.298118</td>\n",
       "      <td>0.415495</td>\n",
       "      <td>0.539678</td>\n",
       "      <td>0.664726</td>\n",
       "      <td>0.771735</td>\n",
       "      <td>0.816813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_1</th>\n",
       "      <td>0.848958</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.848958</td>\n",
       "      <td>0.714361</td>\n",
       "      <td>0.577899</td>\n",
       "      <td>0.447423</td>\n",
       "      <td>0.325441</td>\n",
       "      <td>0.210614</td>\n",
       "      <td>0.103830</td>\n",
       "      <td>0.001354</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.177238</td>\n",
       "      <td>-0.105362</td>\n",
       "      <td>-0.014754</td>\n",
       "      <td>0.084546</td>\n",
       "      <td>0.188584</td>\n",
       "      <td>0.298118</td>\n",
       "      <td>0.415494</td>\n",
       "      <td>0.539678</td>\n",
       "      <td>0.664726</td>\n",
       "      <td>0.771735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_2</th>\n",
       "      <td>0.714361</td>\n",
       "      <td>0.848958</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.848959</td>\n",
       "      <td>0.714362</td>\n",
       "      <td>0.577899</td>\n",
       "      <td>0.447424</td>\n",
       "      <td>0.325441</td>\n",
       "      <td>0.210616</td>\n",
       "      <td>0.103829</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.223379</td>\n",
       "      <td>-0.177227</td>\n",
       "      <td>-0.105352</td>\n",
       "      <td>-0.014751</td>\n",
       "      <td>0.084549</td>\n",
       "      <td>0.188586</td>\n",
       "      <td>0.298119</td>\n",
       "      <td>0.415496</td>\n",
       "      <td>0.539679</td>\n",
       "      <td>0.664727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_3</th>\n",
       "      <td>0.577899</td>\n",
       "      <td>0.714361</td>\n",
       "      <td>0.848959</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.848959</td>\n",
       "      <td>0.714363</td>\n",
       "      <td>0.577900</td>\n",
       "      <td>0.447424</td>\n",
       "      <td>0.325443</td>\n",
       "      <td>0.210615</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.238709</td>\n",
       "      <td>-0.223370</td>\n",
       "      <td>-0.177219</td>\n",
       "      <td>-0.105349</td>\n",
       "      <td>-0.014748</td>\n",
       "      <td>0.084551</td>\n",
       "      <td>0.188588</td>\n",
       "      <td>0.298121</td>\n",
       "      <td>0.415497</td>\n",
       "      <td>0.539681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_4</th>\n",
       "      <td>0.447423</td>\n",
       "      <td>0.577899</td>\n",
       "      <td>0.714362</td>\n",
       "      <td>0.848959</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.848959</td>\n",
       "      <td>0.714363</td>\n",
       "      <td>0.577900</td>\n",
       "      <td>0.447427</td>\n",
       "      <td>0.325441</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.222328</td>\n",
       "      <td>-0.238698</td>\n",
       "      <td>-0.223360</td>\n",
       "      <td>-0.177216</td>\n",
       "      <td>-0.105347</td>\n",
       "      <td>-0.014747</td>\n",
       "      <td>0.084552</td>\n",
       "      <td>0.188589</td>\n",
       "      <td>0.298122</td>\n",
       "      <td>0.415498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_5</th>\n",
       "      <td>0.325441</td>\n",
       "      <td>0.447423</td>\n",
       "      <td>0.577899</td>\n",
       "      <td>0.714363</td>\n",
       "      <td>0.848959</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.848960</td>\n",
       "      <td>0.714363</td>\n",
       "      <td>0.577902</td>\n",
       "      <td>0.447425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.174927</td>\n",
       "      <td>-0.222319</td>\n",
       "      <td>-0.238690</td>\n",
       "      <td>-0.223358</td>\n",
       "      <td>-0.177214</td>\n",
       "      <td>-0.105345</td>\n",
       "      <td>-0.014746</td>\n",
       "      <td>0.084553</td>\n",
       "      <td>0.188590</td>\n",
       "      <td>0.298124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_6</th>\n",
       "      <td>0.210614</td>\n",
       "      <td>0.325441</td>\n",
       "      <td>0.447424</td>\n",
       "      <td>0.577900</td>\n",
       "      <td>0.714363</td>\n",
       "      <td>0.848960</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.848959</td>\n",
       "      <td>0.714365</td>\n",
       "      <td>0.577901</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.102228</td>\n",
       "      <td>-0.174920</td>\n",
       "      <td>-0.222312</td>\n",
       "      <td>-0.238687</td>\n",
       "      <td>-0.223355</td>\n",
       "      <td>-0.177212</td>\n",
       "      <td>-0.105343</td>\n",
       "      <td>-0.014744</td>\n",
       "      <td>0.084555</td>\n",
       "      <td>0.188592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_7</th>\n",
       "      <td>0.103827</td>\n",
       "      <td>0.210614</td>\n",
       "      <td>0.325441</td>\n",
       "      <td>0.447424</td>\n",
       "      <td>0.577900</td>\n",
       "      <td>0.714363</td>\n",
       "      <td>0.848959</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.848960</td>\n",
       "      <td>0.714365</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012312</td>\n",
       "      <td>-0.102230</td>\n",
       "      <td>-0.174921</td>\n",
       "      <td>-0.222312</td>\n",
       "      <td>-0.238687</td>\n",
       "      <td>-0.223355</td>\n",
       "      <td>-0.177212</td>\n",
       "      <td>-0.105343</td>\n",
       "      <td>-0.014745</td>\n",
       "      <td>0.084555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_8</th>\n",
       "      <td>0.001355</td>\n",
       "      <td>0.103830</td>\n",
       "      <td>0.210616</td>\n",
       "      <td>0.325443</td>\n",
       "      <td>0.447427</td>\n",
       "      <td>0.577902</td>\n",
       "      <td>0.714365</td>\n",
       "      <td>0.848960</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.848963</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086367</td>\n",
       "      <td>-0.012322</td>\n",
       "      <td>-0.102237</td>\n",
       "      <td>-0.174919</td>\n",
       "      <td>-0.222310</td>\n",
       "      <td>-0.238684</td>\n",
       "      <td>-0.223352</td>\n",
       "      <td>-0.177209</td>\n",
       "      <td>-0.105342</td>\n",
       "      <td>-0.014742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_9</th>\n",
       "      <td>-0.091820</td>\n",
       "      <td>0.001354</td>\n",
       "      <td>0.103829</td>\n",
       "      <td>0.210615</td>\n",
       "      <td>0.325441</td>\n",
       "      <td>0.447425</td>\n",
       "      <td>0.577901</td>\n",
       "      <td>0.714365</td>\n",
       "      <td>0.848963</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.190507</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>-0.012321</td>\n",
       "      <td>-0.102239</td>\n",
       "      <td>-0.174920</td>\n",
       "      <td>-0.222312</td>\n",
       "      <td>-0.238686</td>\n",
       "      <td>-0.223354</td>\n",
       "      <td>-0.177210</td>\n",
       "      <td>-0.105343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_10</th>\n",
       "      <td>-0.166196</td>\n",
       "      <td>-0.091820</td>\n",
       "      <td>0.001354</td>\n",
       "      <td>0.103828</td>\n",
       "      <td>0.210615</td>\n",
       "      <td>0.325441</td>\n",
       "      <td>0.447425</td>\n",
       "      <td>0.577901</td>\n",
       "      <td>0.714366</td>\n",
       "      <td>0.848963</td>\n",
       "      <td>...</td>\n",
       "      <td>0.301190</td>\n",
       "      <td>0.190509</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>-0.012321</td>\n",
       "      <td>-0.102239</td>\n",
       "      <td>-0.174920</td>\n",
       "      <td>-0.222312</td>\n",
       "      <td>-0.238686</td>\n",
       "      <td>-0.223354</td>\n",
       "      <td>-0.177210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_11</th>\n",
       "      <td>-0.213722</td>\n",
       "      <td>-0.166194</td>\n",
       "      <td>-0.091817</td>\n",
       "      <td>0.001356</td>\n",
       "      <td>0.103831</td>\n",
       "      <td>0.210617</td>\n",
       "      <td>0.325442</td>\n",
       "      <td>0.447425</td>\n",
       "      <td>0.577900</td>\n",
       "      <td>0.714367</td>\n",
       "      <td>...</td>\n",
       "      <td>0.419258</td>\n",
       "      <td>0.301188</td>\n",
       "      <td>0.190506</td>\n",
       "      <td>0.086373</td>\n",
       "      <td>-0.012318</td>\n",
       "      <td>-0.102236</td>\n",
       "      <td>-0.174917</td>\n",
       "      <td>-0.222309</td>\n",
       "      <td>-0.238684</td>\n",
       "      <td>-0.223351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_12</th>\n",
       "      <td>-0.230491</td>\n",
       "      <td>-0.213723</td>\n",
       "      <td>-0.166195</td>\n",
       "      <td>-0.091819</td>\n",
       "      <td>0.001354</td>\n",
       "      <td>0.103830</td>\n",
       "      <td>0.210616</td>\n",
       "      <td>0.325442</td>\n",
       "      <td>0.447425</td>\n",
       "      <td>0.577900</td>\n",
       "      <td>...</td>\n",
       "      <td>0.544814</td>\n",
       "      <td>0.419259</td>\n",
       "      <td>0.301186</td>\n",
       "      <td>0.190504</td>\n",
       "      <td>0.086371</td>\n",
       "      <td>-0.012320</td>\n",
       "      <td>-0.102237</td>\n",
       "      <td>-0.174918</td>\n",
       "      <td>-0.222310</td>\n",
       "      <td>-0.238685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_13</th>\n",
       "      <td>-0.215542</td>\n",
       "      <td>-0.230493</td>\n",
       "      <td>-0.213724</td>\n",
       "      <td>-0.166197</td>\n",
       "      <td>-0.091820</td>\n",
       "      <td>0.001353</td>\n",
       "      <td>0.103828</td>\n",
       "      <td>0.210616</td>\n",
       "      <td>0.325443</td>\n",
       "      <td>0.447425</td>\n",
       "      <td>...</td>\n",
       "      <td>0.675278</td>\n",
       "      <td>0.544816</td>\n",
       "      <td>0.419258</td>\n",
       "      <td>0.301183</td>\n",
       "      <td>0.190502</td>\n",
       "      <td>0.086369</td>\n",
       "      <td>-0.012322</td>\n",
       "      <td>-0.102239</td>\n",
       "      <td>-0.174920</td>\n",
       "      <td>-0.222311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_14</th>\n",
       "      <td>-0.169586</td>\n",
       "      <td>-0.215543</td>\n",
       "      <td>-0.230493</td>\n",
       "      <td>-0.213724</td>\n",
       "      <td>-0.166198</td>\n",
       "      <td>-0.091821</td>\n",
       "      <td>0.001352</td>\n",
       "      <td>0.103829</td>\n",
       "      <td>0.210617</td>\n",
       "      <td>0.325442</td>\n",
       "      <td>...</td>\n",
       "      <td>0.791014</td>\n",
       "      <td>0.675287</td>\n",
       "      <td>0.544820</td>\n",
       "      <td>0.419256</td>\n",
       "      <td>0.301182</td>\n",
       "      <td>0.190501</td>\n",
       "      <td>0.086368</td>\n",
       "      <td>-0.012323</td>\n",
       "      <td>-0.102239</td>\n",
       "      <td>-0.174921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_15</th>\n",
       "      <td>-0.095747</td>\n",
       "      <td>-0.169582</td>\n",
       "      <td>-0.215539</td>\n",
       "      <td>-0.230490</td>\n",
       "      <td>-0.213720</td>\n",
       "      <td>-0.166194</td>\n",
       "      <td>-0.091819</td>\n",
       "      <td>0.001352</td>\n",
       "      <td>0.103826</td>\n",
       "      <td>0.210618</td>\n",
       "      <td>...</td>\n",
       "      <td>0.844906</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.675287</td>\n",
       "      <td>0.544823</td>\n",
       "      <td>0.419259</td>\n",
       "      <td>0.301186</td>\n",
       "      <td>0.190505</td>\n",
       "      <td>0.086372</td>\n",
       "      <td>-0.012319</td>\n",
       "      <td>-0.102236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_16</th>\n",
       "      <td>-0.004988</td>\n",
       "      <td>-0.095747</td>\n",
       "      <td>-0.169581</td>\n",
       "      <td>-0.215538</td>\n",
       "      <td>-0.230489</td>\n",
       "      <td>-0.213719</td>\n",
       "      <td>-0.166194</td>\n",
       "      <td>-0.091819</td>\n",
       "      <td>0.001351</td>\n",
       "      <td>0.103826</td>\n",
       "      <td>...</td>\n",
       "      <td>0.797087</td>\n",
       "      <td>0.844914</td>\n",
       "      <td>0.791020</td>\n",
       "      <td>0.675287</td>\n",
       "      <td>0.544823</td>\n",
       "      <td>0.419260</td>\n",
       "      <td>0.301187</td>\n",
       "      <td>0.190506</td>\n",
       "      <td>0.086373</td>\n",
       "      <td>-0.012318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_17</th>\n",
       "      <td>0.094327</td>\n",
       "      <td>-0.004988</td>\n",
       "      <td>-0.095745</td>\n",
       "      <td>-0.169579</td>\n",
       "      <td>-0.215537</td>\n",
       "      <td>-0.230487</td>\n",
       "      <td>-0.213718</td>\n",
       "      <td>-0.166194</td>\n",
       "      <td>-0.091817</td>\n",
       "      <td>0.001350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.684612</td>\n",
       "      <td>0.797104</td>\n",
       "      <td>0.844925</td>\n",
       "      <td>0.791020</td>\n",
       "      <td>0.675287</td>\n",
       "      <td>0.544824</td>\n",
       "      <td>0.419260</td>\n",
       "      <td>0.301188</td>\n",
       "      <td>0.190507</td>\n",
       "      <td>0.086374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_18</th>\n",
       "      <td>0.199608</td>\n",
       "      <td>0.094326</td>\n",
       "      <td>-0.004987</td>\n",
       "      <td>-0.095743</td>\n",
       "      <td>-0.169579</td>\n",
       "      <td>-0.215537</td>\n",
       "      <td>-0.230487</td>\n",
       "      <td>-0.213718</td>\n",
       "      <td>-0.166191</td>\n",
       "      <td>-0.091818</td>\n",
       "      <td>...</td>\n",
       "      <td>0.556322</td>\n",
       "      <td>0.684633</td>\n",
       "      <td>0.797118</td>\n",
       "      <td>0.844925</td>\n",
       "      <td>0.791020</td>\n",
       "      <td>0.675287</td>\n",
       "      <td>0.544824</td>\n",
       "      <td>0.419260</td>\n",
       "      <td>0.301189</td>\n",
       "      <td>0.190508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_19</th>\n",
       "      <td>0.311368</td>\n",
       "      <td>0.199607</td>\n",
       "      <td>0.094327</td>\n",
       "      <td>-0.004986</td>\n",
       "      <td>-0.095743</td>\n",
       "      <td>-0.169579</td>\n",
       "      <td>-0.215536</td>\n",
       "      <td>-0.230487</td>\n",
       "      <td>-0.213715</td>\n",
       "      <td>-0.166192</td>\n",
       "      <td>...</td>\n",
       "      <td>0.430964</td>\n",
       "      <td>0.556343</td>\n",
       "      <td>0.684648</td>\n",
       "      <td>0.797119</td>\n",
       "      <td>0.844925</td>\n",
       "      <td>0.791020</td>\n",
       "      <td>0.675287</td>\n",
       "      <td>0.544824</td>\n",
       "      <td>0.419261</td>\n",
       "      <td>0.301189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_20</th>\n",
       "      <td>0.430912</td>\n",
       "      <td>0.311368</td>\n",
       "      <td>0.199609</td>\n",
       "      <td>0.094328</td>\n",
       "      <td>-0.004985</td>\n",
       "      <td>-0.095743</td>\n",
       "      <td>-0.169577</td>\n",
       "      <td>-0.215536</td>\n",
       "      <td>-0.230484</td>\n",
       "      <td>-0.213717</td>\n",
       "      <td>...</td>\n",
       "      <td>0.311430</td>\n",
       "      <td>0.430984</td>\n",
       "      <td>0.556357</td>\n",
       "      <td>0.684648</td>\n",
       "      <td>0.797119</td>\n",
       "      <td>0.844926</td>\n",
       "      <td>0.791020</td>\n",
       "      <td>0.675288</td>\n",
       "      <td>0.544825</td>\n",
       "      <td>0.419262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_21</th>\n",
       "      <td>0.556271</td>\n",
       "      <td>0.430912</td>\n",
       "      <td>0.311369</td>\n",
       "      <td>0.199610</td>\n",
       "      <td>0.094329</td>\n",
       "      <td>-0.004984</td>\n",
       "      <td>-0.095741</td>\n",
       "      <td>-0.169577</td>\n",
       "      <td>-0.215533</td>\n",
       "      <td>-0.230485</td>\n",
       "      <td>...</td>\n",
       "      <td>0.199684</td>\n",
       "      <td>0.311446</td>\n",
       "      <td>0.430996</td>\n",
       "      <td>0.556358</td>\n",
       "      <td>0.684649</td>\n",
       "      <td>0.797120</td>\n",
       "      <td>0.844926</td>\n",
       "      <td>0.791021</td>\n",
       "      <td>0.675288</td>\n",
       "      <td>0.544826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_22</th>\n",
       "      <td>0.684564</td>\n",
       "      <td>0.556271</td>\n",
       "      <td>0.430912</td>\n",
       "      <td>0.311370</td>\n",
       "      <td>0.199611</td>\n",
       "      <td>0.094330</td>\n",
       "      <td>-0.004983</td>\n",
       "      <td>-0.095741</td>\n",
       "      <td>-0.169575</td>\n",
       "      <td>-0.215535</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094402</td>\n",
       "      <td>0.199698</td>\n",
       "      <td>0.311457</td>\n",
       "      <td>0.430997</td>\n",
       "      <td>0.556359</td>\n",
       "      <td>0.684649</td>\n",
       "      <td>0.797120</td>\n",
       "      <td>0.844926</td>\n",
       "      <td>0.791021</td>\n",
       "      <td>0.675289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_23</th>\n",
       "      <td>0.797043</td>\n",
       "      <td>0.684564</td>\n",
       "      <td>0.556272</td>\n",
       "      <td>0.430914</td>\n",
       "      <td>0.311371</td>\n",
       "      <td>0.199612</td>\n",
       "      <td>0.094331</td>\n",
       "      <td>-0.004984</td>\n",
       "      <td>-0.095739</td>\n",
       "      <td>-0.169576</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004908</td>\n",
       "      <td>0.094416</td>\n",
       "      <td>0.199709</td>\n",
       "      <td>0.311458</td>\n",
       "      <td>0.430998</td>\n",
       "      <td>0.556359</td>\n",
       "      <td>0.684650</td>\n",
       "      <td>0.797120</td>\n",
       "      <td>0.844926</td>\n",
       "      <td>0.791021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_24</th>\n",
       "      <td>0.844869</td>\n",
       "      <td>0.797043</td>\n",
       "      <td>0.684565</td>\n",
       "      <td>0.556273</td>\n",
       "      <td>0.430915</td>\n",
       "      <td>0.311372</td>\n",
       "      <td>0.199613</td>\n",
       "      <td>0.094331</td>\n",
       "      <td>-0.004982</td>\n",
       "      <td>-0.095740</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.095659</td>\n",
       "      <td>-0.004897</td>\n",
       "      <td>0.094426</td>\n",
       "      <td>0.199711</td>\n",
       "      <td>0.311461</td>\n",
       "      <td>0.431000</td>\n",
       "      <td>0.556360</td>\n",
       "      <td>0.684651</td>\n",
       "      <td>0.797121</td>\n",
       "      <td>0.844926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_25</th>\n",
       "      <td>0.790964</td>\n",
       "      <td>0.844868</td>\n",
       "      <td>0.797044</td>\n",
       "      <td>0.684566</td>\n",
       "      <td>0.556274</td>\n",
       "      <td>0.430916</td>\n",
       "      <td>0.311373</td>\n",
       "      <td>0.199613</td>\n",
       "      <td>0.094334</td>\n",
       "      <td>-0.004983</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.169491</td>\n",
       "      <td>-0.095645</td>\n",
       "      <td>-0.004885</td>\n",
       "      <td>0.094427</td>\n",
       "      <td>0.199713</td>\n",
       "      <td>0.311461</td>\n",
       "      <td>0.431000</td>\n",
       "      <td>0.556361</td>\n",
       "      <td>0.684651</td>\n",
       "      <td>0.797121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_26</th>\n",
       "      <td>0.675231</td>\n",
       "      <td>0.790963</td>\n",
       "      <td>0.844869</td>\n",
       "      <td>0.797044</td>\n",
       "      <td>0.684566</td>\n",
       "      <td>0.556274</td>\n",
       "      <td>0.430917</td>\n",
       "      <td>0.311373</td>\n",
       "      <td>0.199617</td>\n",
       "      <td>0.094333</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.215448</td>\n",
       "      <td>-0.169479</td>\n",
       "      <td>-0.095634</td>\n",
       "      <td>-0.004884</td>\n",
       "      <td>0.094429</td>\n",
       "      <td>0.199713</td>\n",
       "      <td>0.311461</td>\n",
       "      <td>0.431000</td>\n",
       "      <td>0.556362</td>\n",
       "      <td>0.684652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_27</th>\n",
       "      <td>0.544773</td>\n",
       "      <td>0.675231</td>\n",
       "      <td>0.790964</td>\n",
       "      <td>0.844869</td>\n",
       "      <td>0.797045</td>\n",
       "      <td>0.684567</td>\n",
       "      <td>0.556275</td>\n",
       "      <td>0.430916</td>\n",
       "      <td>0.311375</td>\n",
       "      <td>0.199615</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.230402</td>\n",
       "      <td>-0.215440</td>\n",
       "      <td>-0.169471</td>\n",
       "      <td>-0.095631</td>\n",
       "      <td>-0.004881</td>\n",
       "      <td>0.094431</td>\n",
       "      <td>0.199715</td>\n",
       "      <td>0.311463</td>\n",
       "      <td>0.431002</td>\n",
       "      <td>0.556363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_28</th>\n",
       "      <td>0.419210</td>\n",
       "      <td>0.544773</td>\n",
       "      <td>0.675232</td>\n",
       "      <td>0.790964</td>\n",
       "      <td>0.844869</td>\n",
       "      <td>0.797045</td>\n",
       "      <td>0.684567</td>\n",
       "      <td>0.556274</td>\n",
       "      <td>0.430919</td>\n",
       "      <td>0.311374</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.213634</td>\n",
       "      <td>-0.230392</td>\n",
       "      <td>-0.215431</td>\n",
       "      <td>-0.169468</td>\n",
       "      <td>-0.095629</td>\n",
       "      <td>-0.004879</td>\n",
       "      <td>0.094432</td>\n",
       "      <td>0.199716</td>\n",
       "      <td>0.311464</td>\n",
       "      <td>0.431003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_29</th>\n",
       "      <td>0.301134</td>\n",
       "      <td>0.419211</td>\n",
       "      <td>0.544774</td>\n",
       "      <td>0.675232</td>\n",
       "      <td>0.790965</td>\n",
       "      <td>0.844870</td>\n",
       "      <td>0.797045</td>\n",
       "      <td>0.684567</td>\n",
       "      <td>0.556276</td>\n",
       "      <td>0.430918</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.166107</td>\n",
       "      <td>-0.213629</td>\n",
       "      <td>-0.230386</td>\n",
       "      <td>-0.215428</td>\n",
       "      <td>-0.169465</td>\n",
       "      <td>-0.095626</td>\n",
       "      <td>-0.004877</td>\n",
       "      <td>0.094434</td>\n",
       "      <td>0.199718</td>\n",
       "      <td>0.311466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_30</th>\n",
       "      <td>0.190449</td>\n",
       "      <td>0.301136</td>\n",
       "      <td>0.419214</td>\n",
       "      <td>0.544776</td>\n",
       "      <td>0.675235</td>\n",
       "      <td>0.790966</td>\n",
       "      <td>0.844870</td>\n",
       "      <td>0.797044</td>\n",
       "      <td>0.684566</td>\n",
       "      <td>0.556274</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.091736</td>\n",
       "      <td>-0.166103</td>\n",
       "      <td>-0.213624</td>\n",
       "      <td>-0.230382</td>\n",
       "      <td>-0.215423</td>\n",
       "      <td>-0.169461</td>\n",
       "      <td>-0.095622</td>\n",
       "      <td>-0.004873</td>\n",
       "      <td>0.094438</td>\n",
       "      <td>0.199721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_31</th>\n",
       "      <td>0.086315</td>\n",
       "      <td>0.190453</td>\n",
       "      <td>0.301140</td>\n",
       "      <td>0.419217</td>\n",
       "      <td>0.544781</td>\n",
       "      <td>0.675239</td>\n",
       "      <td>0.790969</td>\n",
       "      <td>0.844870</td>\n",
       "      <td>0.797042</td>\n",
       "      <td>0.684567</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001421</td>\n",
       "      <td>-0.091740</td>\n",
       "      <td>-0.166105</td>\n",
       "      <td>-0.213619</td>\n",
       "      <td>-0.230376</td>\n",
       "      <td>-0.215417</td>\n",
       "      <td>-0.169455</td>\n",
       "      <td>-0.095617</td>\n",
       "      <td>-0.004869</td>\n",
       "      <td>0.094443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_32</th>\n",
       "      <td>-0.012363</td>\n",
       "      <td>0.086326</td>\n",
       "      <td>0.190464</td>\n",
       "      <td>0.301150</td>\n",
       "      <td>0.419232</td>\n",
       "      <td>0.544794</td>\n",
       "      <td>0.675251</td>\n",
       "      <td>0.790975</td>\n",
       "      <td>0.844870</td>\n",
       "      <td>0.797051</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103879</td>\n",
       "      <td>0.001402</td>\n",
       "      <td>-0.091753</td>\n",
       "      <td>-0.166097</td>\n",
       "      <td>-0.213611</td>\n",
       "      <td>-0.230367</td>\n",
       "      <td>-0.215408</td>\n",
       "      <td>-0.169445</td>\n",
       "      <td>-0.095608</td>\n",
       "      <td>-0.004859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_33</th>\n",
       "      <td>-0.102279</td>\n",
       "      <td>-0.012356</td>\n",
       "      <td>0.086334</td>\n",
       "      <td>0.190471</td>\n",
       "      <td>0.301159</td>\n",
       "      <td>0.419239</td>\n",
       "      <td>0.544800</td>\n",
       "      <td>0.675250</td>\n",
       "      <td>0.790972</td>\n",
       "      <td>0.844872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.210665</td>\n",
       "      <td>0.103873</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>-0.091745</td>\n",
       "      <td>-0.166089</td>\n",
       "      <td>-0.213602</td>\n",
       "      <td>-0.230358</td>\n",
       "      <td>-0.215398</td>\n",
       "      <td>-0.169437</td>\n",
       "      <td>-0.095600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_34</th>\n",
       "      <td>-0.174969</td>\n",
       "      <td>-0.102269</td>\n",
       "      <td>-0.012346</td>\n",
       "      <td>0.086343</td>\n",
       "      <td>0.190483</td>\n",
       "      <td>0.301170</td>\n",
       "      <td>0.419248</td>\n",
       "      <td>0.544802</td>\n",
       "      <td>0.675248</td>\n",
       "      <td>0.790979</td>\n",
       "      <td>...</td>\n",
       "      <td>0.325471</td>\n",
       "      <td>0.210653</td>\n",
       "      <td>0.103865</td>\n",
       "      <td>0.001409</td>\n",
       "      <td>-0.091736</td>\n",
       "      <td>-0.166078</td>\n",
       "      <td>-0.213591</td>\n",
       "      <td>-0.230347</td>\n",
       "      <td>-0.215390</td>\n",
       "      <td>-0.169427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_35</th>\n",
       "      <td>-0.222359</td>\n",
       "      <td>-0.174958</td>\n",
       "      <td>-0.102258</td>\n",
       "      <td>-0.012335</td>\n",
       "      <td>0.086361</td>\n",
       "      <td>0.190501</td>\n",
       "      <td>0.301187</td>\n",
       "      <td>0.419257</td>\n",
       "      <td>0.544803</td>\n",
       "      <td>0.675267</td>\n",
       "      <td>...</td>\n",
       "      <td>0.447433</td>\n",
       "      <td>0.325450</td>\n",
       "      <td>0.210638</td>\n",
       "      <td>0.103881</td>\n",
       "      <td>0.001422</td>\n",
       "      <td>-0.091721</td>\n",
       "      <td>-0.166065</td>\n",
       "      <td>-0.213579</td>\n",
       "      <td>-0.230339</td>\n",
       "      <td>-0.215380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_36</th>\n",
       "      <td>-0.238731</td>\n",
       "      <td>-0.222351</td>\n",
       "      <td>-0.174950</td>\n",
       "      <td>-0.102251</td>\n",
       "      <td>-0.012325</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>0.190508</td>\n",
       "      <td>0.301188</td>\n",
       "      <td>0.419253</td>\n",
       "      <td>0.544809</td>\n",
       "      <td>...</td>\n",
       "      <td>0.577899</td>\n",
       "      <td>0.447423</td>\n",
       "      <td>0.325443</td>\n",
       "      <td>0.210648</td>\n",
       "      <td>0.103890</td>\n",
       "      <td>0.001432</td>\n",
       "      <td>-0.091711</td>\n",
       "      <td>-0.166055</td>\n",
       "      <td>-0.213572</td>\n",
       "      <td>-0.230332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_37</th>\n",
       "      <td>-0.223395</td>\n",
       "      <td>-0.238724</td>\n",
       "      <td>-0.222345</td>\n",
       "      <td>-0.174944</td>\n",
       "      <td>-0.102243</td>\n",
       "      <td>-0.012318</td>\n",
       "      <td>0.086376</td>\n",
       "      <td>0.190508</td>\n",
       "      <td>0.301184</td>\n",
       "      <td>0.419257</td>\n",
       "      <td>...</td>\n",
       "      <td>0.714372</td>\n",
       "      <td>0.577893</td>\n",
       "      <td>0.447418</td>\n",
       "      <td>0.325452</td>\n",
       "      <td>0.210656</td>\n",
       "      <td>0.103899</td>\n",
       "      <td>0.001441</td>\n",
       "      <td>-0.091702</td>\n",
       "      <td>-0.166049</td>\n",
       "      <td>-0.213565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_38</th>\n",
       "      <td>-0.177247</td>\n",
       "      <td>-0.223387</td>\n",
       "      <td>-0.238716</td>\n",
       "      <td>-0.222338</td>\n",
       "      <td>-0.174935</td>\n",
       "      <td>-0.102235</td>\n",
       "      <td>-0.012311</td>\n",
       "      <td>0.086375</td>\n",
       "      <td>0.190503</td>\n",
       "      <td>0.301187</td>\n",
       "      <td>...</td>\n",
       "      <td>0.848968</td>\n",
       "      <td>0.714369</td>\n",
       "      <td>0.577889</td>\n",
       "      <td>0.447429</td>\n",
       "      <td>0.325462</td>\n",
       "      <td>0.210668</td>\n",
       "      <td>0.103911</td>\n",
       "      <td>0.001452</td>\n",
       "      <td>-0.091694</td>\n",
       "      <td>-0.166040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_39</th>\n",
       "      <td>-0.105373</td>\n",
       "      <td>-0.177238</td>\n",
       "      <td>-0.223379</td>\n",
       "      <td>-0.238709</td>\n",
       "      <td>-0.222328</td>\n",
       "      <td>-0.174927</td>\n",
       "      <td>-0.102228</td>\n",
       "      <td>-0.012312</td>\n",
       "      <td>0.086367</td>\n",
       "      <td>0.190507</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.848965</td>\n",
       "      <td>0.714366</td>\n",
       "      <td>0.577905</td>\n",
       "      <td>0.447443</td>\n",
       "      <td>0.325478</td>\n",
       "      <td>0.210683</td>\n",
       "      <td>0.103925</td>\n",
       "      <td>0.001462</td>\n",
       "      <td>-0.091684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_40</th>\n",
       "      <td>-0.014764</td>\n",
       "      <td>-0.105362</td>\n",
       "      <td>-0.177227</td>\n",
       "      <td>-0.223370</td>\n",
       "      <td>-0.238698</td>\n",
       "      <td>-0.222319</td>\n",
       "      <td>-0.174920</td>\n",
       "      <td>-0.102230</td>\n",
       "      <td>-0.012322</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.848965</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.848964</td>\n",
       "      <td>0.714384</td>\n",
       "      <td>0.577921</td>\n",
       "      <td>0.447461</td>\n",
       "      <td>0.325495</td>\n",
       "      <td>0.210699</td>\n",
       "      <td>0.103937</td>\n",
       "      <td>0.001475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_41</th>\n",
       "      <td>0.084546</td>\n",
       "      <td>-0.014754</td>\n",
       "      <td>-0.105352</td>\n",
       "      <td>-0.177219</td>\n",
       "      <td>-0.223360</td>\n",
       "      <td>-0.238690</td>\n",
       "      <td>-0.222312</td>\n",
       "      <td>-0.174921</td>\n",
       "      <td>-0.102237</td>\n",
       "      <td>-0.012321</td>\n",
       "      <td>...</td>\n",
       "      <td>0.714366</td>\n",
       "      <td>0.848964</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.848975</td>\n",
       "      <td>0.714394</td>\n",
       "      <td>0.577934</td>\n",
       "      <td>0.447473</td>\n",
       "      <td>0.325508</td>\n",
       "      <td>0.210709</td>\n",
       "      <td>0.103947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_42</th>\n",
       "      <td>0.188583</td>\n",
       "      <td>0.084546</td>\n",
       "      <td>-0.014751</td>\n",
       "      <td>-0.105349</td>\n",
       "      <td>-0.177216</td>\n",
       "      <td>-0.223358</td>\n",
       "      <td>-0.238687</td>\n",
       "      <td>-0.222312</td>\n",
       "      <td>-0.174919</td>\n",
       "      <td>-0.102239</td>\n",
       "      <td>...</td>\n",
       "      <td>0.577905</td>\n",
       "      <td>0.714384</td>\n",
       "      <td>0.848975</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.848976</td>\n",
       "      <td>0.714395</td>\n",
       "      <td>0.577935</td>\n",
       "      <td>0.447474</td>\n",
       "      <td>0.325510</td>\n",
       "      <td>0.210711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_43</th>\n",
       "      <td>0.298118</td>\n",
       "      <td>0.188584</td>\n",
       "      <td>0.084549</td>\n",
       "      <td>-0.014748</td>\n",
       "      <td>-0.105347</td>\n",
       "      <td>-0.177214</td>\n",
       "      <td>-0.223355</td>\n",
       "      <td>-0.238687</td>\n",
       "      <td>-0.222310</td>\n",
       "      <td>-0.174920</td>\n",
       "      <td>...</td>\n",
       "      <td>0.447443</td>\n",
       "      <td>0.577921</td>\n",
       "      <td>0.714394</td>\n",
       "      <td>0.848976</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.848976</td>\n",
       "      <td>0.714396</td>\n",
       "      <td>0.577936</td>\n",
       "      <td>0.447476</td>\n",
       "      <td>0.325512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_44</th>\n",
       "      <td>0.415495</td>\n",
       "      <td>0.298118</td>\n",
       "      <td>0.188586</td>\n",
       "      <td>0.084551</td>\n",
       "      <td>-0.014747</td>\n",
       "      <td>-0.105345</td>\n",
       "      <td>-0.177212</td>\n",
       "      <td>-0.223355</td>\n",
       "      <td>-0.238684</td>\n",
       "      <td>-0.222312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.325478</td>\n",
       "      <td>0.447461</td>\n",
       "      <td>0.577934</td>\n",
       "      <td>0.714395</td>\n",
       "      <td>0.848976</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.848976</td>\n",
       "      <td>0.714396</td>\n",
       "      <td>0.577937</td>\n",
       "      <td>0.447477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_45</th>\n",
       "      <td>0.539678</td>\n",
       "      <td>0.415494</td>\n",
       "      <td>0.298119</td>\n",
       "      <td>0.188588</td>\n",
       "      <td>0.084552</td>\n",
       "      <td>-0.014746</td>\n",
       "      <td>-0.105343</td>\n",
       "      <td>-0.177212</td>\n",
       "      <td>-0.223352</td>\n",
       "      <td>-0.238686</td>\n",
       "      <td>...</td>\n",
       "      <td>0.210683</td>\n",
       "      <td>0.325495</td>\n",
       "      <td>0.447473</td>\n",
       "      <td>0.577935</td>\n",
       "      <td>0.714396</td>\n",
       "      <td>0.848976</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.848977</td>\n",
       "      <td>0.714397</td>\n",
       "      <td>0.577938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_46</th>\n",
       "      <td>0.664726</td>\n",
       "      <td>0.539678</td>\n",
       "      <td>0.415496</td>\n",
       "      <td>0.298121</td>\n",
       "      <td>0.188589</td>\n",
       "      <td>0.084553</td>\n",
       "      <td>-0.014744</td>\n",
       "      <td>-0.105343</td>\n",
       "      <td>-0.177209</td>\n",
       "      <td>-0.223354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103925</td>\n",
       "      <td>0.210699</td>\n",
       "      <td>0.325508</td>\n",
       "      <td>0.447474</td>\n",
       "      <td>0.577936</td>\n",
       "      <td>0.714396</td>\n",
       "      <td>0.848977</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.848977</td>\n",
       "      <td>0.714398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_47</th>\n",
       "      <td>0.771735</td>\n",
       "      <td>0.664726</td>\n",
       "      <td>0.539679</td>\n",
       "      <td>0.415497</td>\n",
       "      <td>0.298122</td>\n",
       "      <td>0.188590</td>\n",
       "      <td>0.084555</td>\n",
       "      <td>-0.014745</td>\n",
       "      <td>-0.105342</td>\n",
       "      <td>-0.177210</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001462</td>\n",
       "      <td>0.103937</td>\n",
       "      <td>0.210709</td>\n",
       "      <td>0.325510</td>\n",
       "      <td>0.447476</td>\n",
       "      <td>0.577937</td>\n",
       "      <td>0.714397</td>\n",
       "      <td>0.848977</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.848978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_48</th>\n",
       "      <td>0.816813</td>\n",
       "      <td>0.771735</td>\n",
       "      <td>0.664727</td>\n",
       "      <td>0.539681</td>\n",
       "      <td>0.415498</td>\n",
       "      <td>0.298124</td>\n",
       "      <td>0.188592</td>\n",
       "      <td>0.084555</td>\n",
       "      <td>-0.014742</td>\n",
       "      <td>-0.105343</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.091684</td>\n",
       "      <td>0.001475</td>\n",
       "      <td>0.103947</td>\n",
       "      <td>0.210711</td>\n",
       "      <td>0.325512</td>\n",
       "      <td>0.447477</td>\n",
       "      <td>0.577938</td>\n",
       "      <td>0.714398</td>\n",
       "      <td>0.848978</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49 rows  49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           calls     lag_1     lag_2     lag_3     lag_4     lag_5     lag_6  \\\n",
       "calls   1.000000  0.848958  0.714361  0.577899  0.447423  0.325441  0.210614   \n",
       "lag_1   0.848958  1.000000  0.848958  0.714361  0.577899  0.447423  0.325441   \n",
       "lag_2   0.714361  0.848958  1.000000  0.848959  0.714362  0.577899  0.447424   \n",
       "lag_3   0.577899  0.714361  0.848959  1.000000  0.848959  0.714363  0.577900   \n",
       "lag_4   0.447423  0.577899  0.714362  0.848959  1.000000  0.848959  0.714363   \n",
       "lag_5   0.325441  0.447423  0.577899  0.714363  0.848959  1.000000  0.848960   \n",
       "lag_6   0.210614  0.325441  0.447424  0.577900  0.714363  0.848960  1.000000   \n",
       "lag_7   0.103827  0.210614  0.325441  0.447424  0.577900  0.714363  0.848959   \n",
       "lag_8   0.001355  0.103830  0.210616  0.325443  0.447427  0.577902  0.714365   \n",
       "lag_9  -0.091820  0.001354  0.103829  0.210615  0.325441  0.447425  0.577901   \n",
       "lag_10 -0.166196 -0.091820  0.001354  0.103828  0.210615  0.325441  0.447425   \n",
       "lag_11 -0.213722 -0.166194 -0.091817  0.001356  0.103831  0.210617  0.325442   \n",
       "lag_12 -0.230491 -0.213723 -0.166195 -0.091819  0.001354  0.103830  0.210616   \n",
       "lag_13 -0.215542 -0.230493 -0.213724 -0.166197 -0.091820  0.001353  0.103828   \n",
       "lag_14 -0.169586 -0.215543 -0.230493 -0.213724 -0.166198 -0.091821  0.001352   \n",
       "lag_15 -0.095747 -0.169582 -0.215539 -0.230490 -0.213720 -0.166194 -0.091819   \n",
       "lag_16 -0.004988 -0.095747 -0.169581 -0.215538 -0.230489 -0.213719 -0.166194   \n",
       "lag_17  0.094327 -0.004988 -0.095745 -0.169579 -0.215537 -0.230487 -0.213718   \n",
       "lag_18  0.199608  0.094326 -0.004987 -0.095743 -0.169579 -0.215537 -0.230487   \n",
       "lag_19  0.311368  0.199607  0.094327 -0.004986 -0.095743 -0.169579 -0.215536   \n",
       "lag_20  0.430912  0.311368  0.199609  0.094328 -0.004985 -0.095743 -0.169577   \n",
       "lag_21  0.556271  0.430912  0.311369  0.199610  0.094329 -0.004984 -0.095741   \n",
       "lag_22  0.684564  0.556271  0.430912  0.311370  0.199611  0.094330 -0.004983   \n",
       "lag_23  0.797043  0.684564  0.556272  0.430914  0.311371  0.199612  0.094331   \n",
       "lag_24  0.844869  0.797043  0.684565  0.556273  0.430915  0.311372  0.199613   \n",
       "lag_25  0.790964  0.844868  0.797044  0.684566  0.556274  0.430916  0.311373   \n",
       "lag_26  0.675231  0.790963  0.844869  0.797044  0.684566  0.556274  0.430917   \n",
       "lag_27  0.544773  0.675231  0.790964  0.844869  0.797045  0.684567  0.556275   \n",
       "lag_28  0.419210  0.544773  0.675232  0.790964  0.844869  0.797045  0.684567   \n",
       "lag_29  0.301134  0.419211  0.544774  0.675232  0.790965  0.844870  0.797045   \n",
       "lag_30  0.190449  0.301136  0.419214  0.544776  0.675235  0.790966  0.844870   \n",
       "lag_31  0.086315  0.190453  0.301140  0.419217  0.544781  0.675239  0.790969   \n",
       "lag_32 -0.012363  0.086326  0.190464  0.301150  0.419232  0.544794  0.675251   \n",
       "lag_33 -0.102279 -0.012356  0.086334  0.190471  0.301159  0.419239  0.544800   \n",
       "lag_34 -0.174969 -0.102269 -0.012346  0.086343  0.190483  0.301170  0.419248   \n",
       "lag_35 -0.222359 -0.174958 -0.102258 -0.012335  0.086361  0.190501  0.301187   \n",
       "lag_36 -0.238731 -0.222351 -0.174950 -0.102251 -0.012325  0.086370  0.190508   \n",
       "lag_37 -0.223395 -0.238724 -0.222345 -0.174944 -0.102243 -0.012318  0.086376   \n",
       "lag_38 -0.177247 -0.223387 -0.238716 -0.222338 -0.174935 -0.102235 -0.012311   \n",
       "lag_39 -0.105373 -0.177238 -0.223379 -0.238709 -0.222328 -0.174927 -0.102228   \n",
       "lag_40 -0.014764 -0.105362 -0.177227 -0.223370 -0.238698 -0.222319 -0.174920   \n",
       "lag_41  0.084546 -0.014754 -0.105352 -0.177219 -0.223360 -0.238690 -0.222312   \n",
       "lag_42  0.188583  0.084546 -0.014751 -0.105349 -0.177216 -0.223358 -0.238687   \n",
       "lag_43  0.298118  0.188584  0.084549 -0.014748 -0.105347 -0.177214 -0.223355   \n",
       "lag_44  0.415495  0.298118  0.188586  0.084551 -0.014747 -0.105345 -0.177212   \n",
       "lag_45  0.539678  0.415494  0.298119  0.188588  0.084552 -0.014746 -0.105343   \n",
       "lag_46  0.664726  0.539678  0.415496  0.298121  0.188589  0.084553 -0.014744   \n",
       "lag_47  0.771735  0.664726  0.539679  0.415497  0.298122  0.188590  0.084555   \n",
       "lag_48  0.816813  0.771735  0.664727  0.539681  0.415498  0.298124  0.188592   \n",
       "\n",
       "           lag_7     lag_8     lag_9  ...    lag_39    lag_40    lag_41  \\\n",
       "calls   0.103827  0.001355 -0.091820  ... -0.105373 -0.014764  0.084546   \n",
       "lag_1   0.210614  0.103830  0.001354  ... -0.177238 -0.105362 -0.014754   \n",
       "lag_2   0.325441  0.210616  0.103829  ... -0.223379 -0.177227 -0.105352   \n",
       "lag_3   0.447424  0.325443  0.210615  ... -0.238709 -0.223370 -0.177219   \n",
       "lag_4   0.577900  0.447427  0.325441  ... -0.222328 -0.238698 -0.223360   \n",
       "lag_5   0.714363  0.577902  0.447425  ... -0.174927 -0.222319 -0.238690   \n",
       "lag_6   0.848959  0.714365  0.577901  ... -0.102228 -0.174920 -0.222312   \n",
       "lag_7   1.000000  0.848960  0.714365  ... -0.012312 -0.102230 -0.174921   \n",
       "lag_8   0.848960  1.000000  0.848963  ...  0.086367 -0.012322 -0.102237   \n",
       "lag_9   0.714365  0.848963  1.000000  ...  0.190507  0.086370 -0.012321   \n",
       "lag_10  0.577901  0.714366  0.848963  ...  0.301190  0.190509  0.086370   \n",
       "lag_11  0.447425  0.577900  0.714367  ...  0.419258  0.301188  0.190506   \n",
       "lag_12  0.325442  0.447425  0.577900  ...  0.544814  0.419259  0.301186   \n",
       "lag_13  0.210616  0.325443  0.447425  ...  0.675278  0.544816  0.419258   \n",
       "lag_14  0.103829  0.210617  0.325442  ...  0.791014  0.675287  0.544820   \n",
       "lag_15  0.001352  0.103826  0.210618  ...  0.844906  0.791017  0.675287   \n",
       "lag_16 -0.091819  0.001351  0.103826  ...  0.797087  0.844914  0.791020   \n",
       "lag_17 -0.166194 -0.091817  0.001350  ...  0.684612  0.797104  0.844925   \n",
       "lag_18 -0.213718 -0.166191 -0.091818  ...  0.556322  0.684633  0.797118   \n",
       "lag_19 -0.230487 -0.213715 -0.166192  ...  0.430964  0.556343  0.684648   \n",
       "lag_20 -0.215536 -0.230484 -0.213717  ...  0.311430  0.430984  0.556357   \n",
       "lag_21 -0.169577 -0.215533 -0.230485  ...  0.199684  0.311446  0.430996   \n",
       "lag_22 -0.095741 -0.169575 -0.215535  ...  0.094402  0.199698  0.311457   \n",
       "lag_23 -0.004984 -0.095739 -0.169576  ... -0.004908  0.094416  0.199709   \n",
       "lag_24  0.094331 -0.004982 -0.095740  ... -0.095659 -0.004897  0.094426   \n",
       "lag_25  0.199613  0.094334 -0.004983  ... -0.169491 -0.095645 -0.004885   \n",
       "lag_26  0.311373  0.199617  0.094333  ... -0.215448 -0.169479 -0.095634   \n",
       "lag_27  0.430916  0.311375  0.199615  ... -0.230402 -0.215440 -0.169471   \n",
       "lag_28  0.556274  0.430919  0.311374  ... -0.213634 -0.230392 -0.215431   \n",
       "lag_29  0.684567  0.556276  0.430918  ... -0.166107 -0.213629 -0.230386   \n",
       "lag_30  0.797044  0.684566  0.556274  ... -0.091736 -0.166103 -0.213624   \n",
       "lag_31  0.844870  0.797042  0.684567  ...  0.001421 -0.091740 -0.166105   \n",
       "lag_32  0.790975  0.844870  0.797051  ...  0.103879  0.001402 -0.091753   \n",
       "lag_33  0.675250  0.790972  0.844872  ...  0.210665  0.103873  0.001399   \n",
       "lag_34  0.544802  0.675248  0.790979  ...  0.325471  0.210653  0.103865   \n",
       "lag_35  0.419257  0.544803  0.675267  ...  0.447433  0.325450  0.210638   \n",
       "lag_36  0.301188  0.419253  0.544809  ...  0.577899  0.447423  0.325443   \n",
       "lag_37  0.190508  0.301184  0.419257  ...  0.714372  0.577893  0.447418   \n",
       "lag_38  0.086375  0.190503  0.301187  ...  0.848968  0.714369  0.577889   \n",
       "lag_39 -0.012312  0.086367  0.190507  ...  1.000000  0.848965  0.714366   \n",
       "lag_40 -0.102230 -0.012322  0.086370  ...  0.848965  1.000000  0.848964   \n",
       "lag_41 -0.174921 -0.102237 -0.012321  ...  0.714366  0.848964  1.000000   \n",
       "lag_42 -0.222312 -0.174919 -0.102239  ...  0.577905  0.714384  0.848975   \n",
       "lag_43 -0.238687 -0.222310 -0.174920  ...  0.447443  0.577921  0.714394   \n",
       "lag_44 -0.223355 -0.238684 -0.222312  ...  0.325478  0.447461  0.577934   \n",
       "lag_45 -0.177212 -0.223352 -0.238686  ...  0.210683  0.325495  0.447473   \n",
       "lag_46 -0.105343 -0.177209 -0.223354  ...  0.103925  0.210699  0.325508   \n",
       "lag_47 -0.014745 -0.105342 -0.177210  ...  0.001462  0.103937  0.210709   \n",
       "lag_48  0.084555 -0.014742 -0.105343  ... -0.091684  0.001475  0.103947   \n",
       "\n",
       "          lag_42    lag_43    lag_44    lag_45    lag_46    lag_47    lag_48  \n",
       "calls   0.188583  0.298118  0.415495  0.539678  0.664726  0.771735  0.816813  \n",
       "lag_1   0.084546  0.188584  0.298118  0.415494  0.539678  0.664726  0.771735  \n",
       "lag_2  -0.014751  0.084549  0.188586  0.298119  0.415496  0.539679  0.664727  \n",
       "lag_3  -0.105349 -0.014748  0.084551  0.188588  0.298121  0.415497  0.539681  \n",
       "lag_4  -0.177216 -0.105347 -0.014747  0.084552  0.188589  0.298122  0.415498  \n",
       "lag_5  -0.223358 -0.177214 -0.105345 -0.014746  0.084553  0.188590  0.298124  \n",
       "lag_6  -0.238687 -0.223355 -0.177212 -0.105343 -0.014744  0.084555  0.188592  \n",
       "lag_7  -0.222312 -0.238687 -0.223355 -0.177212 -0.105343 -0.014745  0.084555  \n",
       "lag_8  -0.174919 -0.222310 -0.238684 -0.223352 -0.177209 -0.105342 -0.014742  \n",
       "lag_9  -0.102239 -0.174920 -0.222312 -0.238686 -0.223354 -0.177210 -0.105343  \n",
       "lag_10 -0.012321 -0.102239 -0.174920 -0.222312 -0.238686 -0.223354 -0.177210  \n",
       "lag_11  0.086373 -0.012318 -0.102236 -0.174917 -0.222309 -0.238684 -0.223351  \n",
       "lag_12  0.190504  0.086371 -0.012320 -0.102237 -0.174918 -0.222310 -0.238685  \n",
       "lag_13  0.301183  0.190502  0.086369 -0.012322 -0.102239 -0.174920 -0.222311  \n",
       "lag_14  0.419256  0.301182  0.190501  0.086368 -0.012323 -0.102239 -0.174921  \n",
       "lag_15  0.544823  0.419259  0.301186  0.190505  0.086372 -0.012319 -0.102236  \n",
       "lag_16  0.675287  0.544823  0.419260  0.301187  0.190506  0.086373 -0.012318  \n",
       "lag_17  0.791020  0.675287  0.544824  0.419260  0.301188  0.190507  0.086374  \n",
       "lag_18  0.844925  0.791020  0.675287  0.544824  0.419260  0.301189  0.190508  \n",
       "lag_19  0.797119  0.844925  0.791020  0.675287  0.544824  0.419261  0.301189  \n",
       "lag_20  0.684648  0.797119  0.844926  0.791020  0.675288  0.544825  0.419262  \n",
       "lag_21  0.556358  0.684649  0.797120  0.844926  0.791021  0.675288  0.544826  \n",
       "lag_22  0.430997  0.556359  0.684649  0.797120  0.844926  0.791021  0.675289  \n",
       "lag_23  0.311458  0.430998  0.556359  0.684650  0.797120  0.844926  0.791021  \n",
       "lag_24  0.199711  0.311461  0.431000  0.556360  0.684651  0.797121  0.844926  \n",
       "lag_25  0.094427  0.199713  0.311461  0.431000  0.556361  0.684651  0.797121  \n",
       "lag_26 -0.004884  0.094429  0.199713  0.311461  0.431000  0.556362  0.684652  \n",
       "lag_27 -0.095631 -0.004881  0.094431  0.199715  0.311463  0.431002  0.556363  \n",
       "lag_28 -0.169468 -0.095629 -0.004879  0.094432  0.199716  0.311464  0.431003  \n",
       "lag_29 -0.215428 -0.169465 -0.095626 -0.004877  0.094434  0.199718  0.311466  \n",
       "lag_30 -0.230382 -0.215423 -0.169461 -0.095622 -0.004873  0.094438  0.199721  \n",
       "lag_31 -0.213619 -0.230376 -0.215417 -0.169455 -0.095617 -0.004869  0.094443  \n",
       "lag_32 -0.166097 -0.213611 -0.230367 -0.215408 -0.169445 -0.095608 -0.004859  \n",
       "lag_33 -0.091745 -0.166089 -0.213602 -0.230358 -0.215398 -0.169437 -0.095600  \n",
       "lag_34  0.001409 -0.091736 -0.166078 -0.213591 -0.230347 -0.215390 -0.169427  \n",
       "lag_35  0.103881  0.001422 -0.091721 -0.166065 -0.213579 -0.230339 -0.215380  \n",
       "lag_36  0.210648  0.103890  0.001432 -0.091711 -0.166055 -0.213572 -0.230332  \n",
       "lag_37  0.325452  0.210656  0.103899  0.001441 -0.091702 -0.166049 -0.213565  \n",
       "lag_38  0.447429  0.325462  0.210668  0.103911  0.001452 -0.091694 -0.166040  \n",
       "lag_39  0.577905  0.447443  0.325478  0.210683  0.103925  0.001462 -0.091684  \n",
       "lag_40  0.714384  0.577921  0.447461  0.325495  0.210699  0.103937  0.001475  \n",
       "lag_41  0.848975  0.714394  0.577934  0.447473  0.325508  0.210709  0.103947  \n",
       "lag_42  1.000000  0.848976  0.714395  0.577935  0.447474  0.325510  0.210711  \n",
       "lag_43  0.848976  1.000000  0.848976  0.714396  0.577936  0.447476  0.325512  \n",
       "lag_44  0.714395  0.848976  1.000000  0.848976  0.714396  0.577937  0.447477  \n",
       "lag_45  0.577935  0.714396  0.848976  1.000000  0.848977  0.714397  0.577938  \n",
       "lag_46  0.447474  0.577936  0.714396  0.848977  1.000000  0.848977  0.714398  \n",
       "lag_47  0.325510  0.447476  0.577937  0.714397  0.848977  1.000000  0.848978  \n",
       "lag_48  0.210711  0.325512  0.447477  0.577938  0.714398  0.848978  1.000000  \n",
       "\n",
       "[49 rows x 49 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = X_.dropna().drop('calls', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = X.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = X_.dropna()['calls'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ind = int(len(y)*.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Regression With Lags -----\n",
      "2009 --> 2010\n",
      "rmse 3.7693269749331217\n",
      "r2 0.42702776226472905\n",
      "max resid 29.18288986337003\n",
      "2010 --> 2011\n",
      "rmse 4.367847771353403\n",
      "r2 0.39352916745044175\n",
      "max resid 37.70959016164434\n",
      "2011 --> 2012\n",
      "rmse 4.513121121967147\n",
      "r2 0.38289401659104416\n",
      "max resid 126.26354242643939\n",
      "2012 --> 2013\n",
      "rmse 4.804020277195866\n",
      "r2 0.5093591595603264\n",
      "max resid 35.26866028585498\n",
      "2013 --> 2014\n",
      "rmse 6.248891650160857\n",
      "r2 0.49631429981022046\n",
      "max resid 151.3967777481542\n",
      "2014 --> 2015\n",
      "rmse 6.538795258238863\n",
      "r2 0.6165287351622348\n",
      "max resid 70.79267581607405\n",
      "2015 --> 2016\n",
      "rmse 9.200039380382208\n",
      "r2 0.5971082083755457\n",
      "max resid 96.3364571486492\n",
      "2016 --> 2017\n",
      "rmse 9.159397358157154\n",
      "r2 0.7544116305634567\n",
      "max resid 85.5076332372751\n",
      "2017 --> 2018\n",
      "rmse 10.880094678124307\n",
      "r2 0.7406458973800923\n",
      "max resid 219.96825970832916\n",
      "2018 --> 2019\n",
      "rmse 11.731135875710885\n",
      "r2 0.7645793439620926\n",
      "max resid 262.655132359646\n",
      "2019 --> 2020\n",
      "rmse 12.62038694526827\n",
      "r2 0.7723444852865617\n",
      "max resid 256.7704331827631\n",
      "2020 --> 2021\n",
      "rmse 15.61509193329624\n",
      "r2 0.6903771271953285\n",
      "max resid 516.1651611423322\n",
      "\n",
      "Mean RMSE: 8.28734576873236\n",
      "Mean r2: 0.5954266528001729\n",
      "Max resid: 157.334767756711\n"
     ]
    }
   ],
   "source": [
    "all_years = df.index.year.unique()\n",
    "rmses = []\n",
    "r2s = []\n",
    "max_resids = []\n",
    "\n",
    "print(\"----- Regression With Lags -----\")\n",
    "for i in range(len(all_years)-2):\n",
    "    print(all_years[i], \"-->\", all_years[i+1])\n",
    "    train_data = X_[X_.index.year == all_years[i]]\n",
    "    test_data = X_[X_.index.year == all_years[i+1]]\n",
    "\n",
    "    X_train = train_data.drop('calls', axis=1).values\n",
    "    y_train = train_data[['calls']].values\n",
    "    X_test = test_data.drop('calls', axis=1).values\n",
    "    y_test = test_data[['calls']].values\n",
    "\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_X.fit(X_train)\n",
    "    scaler_y = StandardScaler()\n",
    "    scaler_y.fit(y_train)\n",
    "\n",
    "    X_train_scaled = scaler_X.transform(X_train)\n",
    "    y_train_scaled = scaler_y.transform(y_train)\n",
    "    X_test_scaled = scaler_X.transform(X_test)\n",
    "    y_test_scaled = scaler_y.transform(y_test)\n",
    "\n",
    "    X_train_scaled = np.clip(X_train_scaled, -3, 3)\n",
    "    X_test_scaled = np.clip(X_test_scaled, -3, 3)\n",
    "\n",
    "    lm = LinearRegression()\n",
    "    lm.fit(X_train_scaled, y_train_scaled)\n",
    "    test_preds = scaler_y.inverse_transform(lm.predict(X_test_scaled))\n",
    "    actual_calls = y_test\n",
    "    rmse, r2, max_resid = evaluate_fit(test_preds, actual_calls)\n",
    "    rmses.append(rmse)\n",
    "    r2s.append(r2)\n",
    "    max_resids.append(max_resid)\n",
    "\n",
    "print()\n",
    "print(\"Mean RMSE:\", np.mean(rmses))\n",
    "print(\"Mean r2:\", np.mean(r2s))\n",
    "print(\"Max resid:\", np.mean(max_resids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Ridge Regression With Lags -----\n",
      "2009 --> 2010\n",
      "rmse 3.769331411390137\n",
      "r2 0.42698291253335185\n",
      "max resid 29.183068121920485\n",
      "2010 --> 2011\n",
      "rmse 4.367843397640271\n",
      "r2 0.3934838538842056\n",
      "max resid 37.70817796081691\n",
      "2011 --> 2012\n",
      "rmse 4.51311126122042\n",
      "r2 0.3828477733372906\n",
      "max resid 126.26311254908566\n",
      "2012 --> 2013\n",
      "rmse 4.8040497562727635\n",
      "r2 0.5093177244728688\n",
      "max resid 35.26874439287556\n",
      "2013 --> 2014\n",
      "rmse 6.248918572469018\n",
      "r2 0.4962761948195026\n",
      "max resid 151.3987206030451\n",
      "2014 --> 2015\n",
      "rmse 6.538813978291443\n",
      "r2 0.6165015169029531\n",
      "max resid 70.79452451354015\n",
      "2015 --> 2016\n",
      "rmse 9.200267332649378\n",
      "r2 0.5970630110872404\n",
      "max resid 96.33739042399566\n",
      "2016 --> 2017\n",
      "rmse 9.159565881930877\n",
      "r2 0.7543824198996067\n",
      "max resid 85.50276644204328\n",
      "2017 --> 2018\n",
      "rmse 10.880152977262053\n",
      "r2 0.74061820435455\n",
      "max resid 219.97660506620207\n",
      "2018 --> 2019\n",
      "rmse 11.731135869216518\n",
      "r2 0.764579342114099\n",
      "max resid 262.65513226088063\n",
      "2019 --> 2020\n",
      "rmse 12.620406161781851\n",
      "r2 0.7723248325276224\n",
      "max resid 256.79445429628197\n",
      "2020 --> 2021\n",
      "rmse 15.615233215696161\n",
      "r2 0.6903412416881687\n",
      "max resid 516.1698553607235\n",
      "\n",
      "Mean RMSE: 8.287402484651741\n",
      "Mean r2: 0.5953932523017883\n",
      "Max resid: 157.3377126659509\n"
     ]
    }
   ],
   "source": [
    "all_years = df.index.year.unique()\n",
    "rmses = []\n",
    "r2s = []\n",
    "max_resids = []\n",
    "\n",
    "print(\"----- Ridge Regression With Lags -----\")\n",
    "for i in range(len(all_years)-2):\n",
    "    print(all_years[i], \"-->\", all_years[i+1])\n",
    "    train_data = X_[X_.index.year == all_years[i]]\n",
    "    test_data = X_[X_.index.year == all_years[i+1]]\n",
    "\n",
    "    X_train = train_data.drop('calls', axis=1).values\n",
    "    y_train = train_data[['calls']].values\n",
    "    X_test = test_data.drop('calls', axis=1).values\n",
    "    y_test = test_data[['calls']].values\n",
    "\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_X.fit(X_train)\n",
    "    scaler_y = StandardScaler()\n",
    "    scaler_y.fit(y_train)\n",
    "\n",
    "    X_train_scaled = scaler_X.transform(X_train)\n",
    "    y_train_scaled = scaler_y.transform(y_train)\n",
    "    X_test_scaled = scaler_X.transform(X_test)\n",
    "    y_test_scaled = scaler_y.transform(y_test)\n",
    "\n",
    "    X_train_scaled = np.clip(X_train_scaled, -3, 3)\n",
    "    X_test_scaled = np.clip(X_test_scaled, -3, 3)\n",
    "\n",
    "    lm = RidgeCV(alphas=np.linspace(0.0001, 1, 30), scoring=\"r2\", cv=TimeSeriesSplit(n_splits=4))\n",
    "    lm.fit(X_train_scaled, y_train_scaled)\n",
    "    test_preds = scaler_y.inverse_transform(lm.predict(X_test_scaled))\n",
    "    actual_calls = y_test\n",
    "    rmse, r2, max_resid = evaluate_fit(test_preds, actual_calls)\n",
    "    rmses.append(rmse)\n",
    "    r2s.append(r2)\n",
    "    max_resids.append(max_resid)\n",
    "\n",
    "print()\n",
    "print(\"Mean RMSE:\", np.mean(rmses))\n",
    "print(\"Mean r2:\", np.mean(r2s))\n",
    "print(\"Max resid:\", np.mean(max_resids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_years = df.index.year.unique()\n",
    "rmses = []\n",
    "r2s = []\n",
    "max_resids = []\n",
    "\n",
    "print(\"----- Ridge Regression With Lags -----\")\n",
    "for i in range(len(all_years)-2):\n",
    "    print(all_years[i], \"-->\", all_years[i+1])\n",
    "    train_data = X_[X_.index.year == all_years[i]]\n",
    "    test_data = X_[X_.index.year == all_years[i+1]]\n",
    "\n",
    "    X_train = train_data.drop('calls', axis=1).values\n",
    "    y_train = train_data[['calls']].values\n",
    "    X_test = test_data.drop('calls', axis=1).values\n",
    "    y_test = test_data[['calls']].values\n",
    "\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_X.fit(X_train)\n",
    "    scaler_y = StandardScaler()\n",
    "    scaler_y.fit(y_train)\n",
    "\n",
    "    X_train_scaled = scaler_X.transform(X_train)\n",
    "    y_train_scaled = scaler_y.transform(y_train)\n",
    "    X_test_scaled = scaler_X.transform(X_test)\n",
    "    y_test_scaled = scaler_y.transform(y_test)\n",
    "\n",
    "    X_train_scaled = np.clip(X_train_scaled, -3, 3)\n",
    "    X_test_scaled = np.clip(X_test_scaled, -3, 3)\n",
    "\n",
    "    lm = RidgeCV(alphas=np.linspace(0.0001, 1, 30), scoring=\"r2\", cv=TimeSeriesSplit(n_splits=4))\n",
    "    lm.fit(X_train_scaled, y_train_scaled)\n",
    "    test_preds = scaler_y.inverse_transform(lm.predict(X_test_scaled))\n",
    "    actual_calls = y_test\n",
    "    rmse, r2, max_resid = evaluate_fit(test_preds, actual_calls)\n",
    "    rmses.append(rmse)\n",
    "    r2s.append(r2)\n",
    "    max_resids.append(max_resid)\n",
    "\n",
    "print()\n",
    "print(\"Mean RMSE:\", np.mean(rmses))\n",
    "print(\"Mean r2:\", np.mean(r2s))\n",
    "print(\"Max resid:\", np.mean(max_resids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def r2(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Vanilla RNN With Lags -----\n",
      "2009 --> 2010\n",
      "Epoch 1/6\n",
      "88/88 [==============================] - 30s 286ms/step - loss: 0.8465\n",
      "Epoch 2/6\n",
      "88/88 [==============================] - 28s 318ms/step - loss: 0.6393\n",
      "Epoch 3/6\n",
      "88/88 [==============================] - 31s 351ms/step - loss: 0.6056\n",
      "Epoch 4/6\n",
      "88/88 [==============================] - 27s 307ms/step - loss: 0.5774\n",
      "Epoch 5/6\n",
      "88/88 [==============================] - 27s 306ms/step - loss: 0.5354\n",
      "Epoch 6/6\n",
      "88/88 [==============================] - 27s 310ms/step - loss: 0.5068\n",
      "274/274 [==============================] - 19s 67ms/step\n",
      "Train\n",
      "273/273 [==============================] - 22s 80ms/step\n",
      "rmse 4.102100142637915\n",
      "r2 -0.4471552673312975\n",
      "max resid 33.528743743896484\n",
      "Test\n",
      "rmse 4.4683125507703565\n",
      "r2 -0.029087220966261373\n",
      "max resid 31.00409460067749\n",
      "2010 --> 2011\n",
      "Epoch 1/6\n",
      "88/88 [==============================] - 37s 295ms/step - loss: 0.8731\n",
      "Epoch 2/6\n",
      "88/88 [==============================] - 24s 269ms/step - loss: 0.6391\n",
      "Epoch 3/6\n",
      "88/88 [==============================] - 24s 273ms/step - loss: 0.5897\n",
      "Epoch 4/6\n",
      "88/88 [==============================] - 24s 270ms/step - loss: 0.5626\n",
      "Epoch 5/6\n",
      "88/88 [==============================] - 26s 297ms/step - loss: 0.5387\n",
      "Epoch 6/6\n",
      "88/88 [==============================] - 25s 283ms/step - loss: 0.5106\n",
      "274/274 [==============================] - 16s 58ms/step\n",
      "Train\n",
      "274/274 [==============================] - 15s 56ms/step\n",
      "rmse 4.568001564185647\n",
      "r2 -0.852344141144143\n",
      "max resid 28.574995040893555\n",
      "Test\n",
      "rmse 6.028136331982021\n",
      "r2 -0.34243933361047074\n",
      "max resid 46.06041830778122\n",
      "2011 --> 2012\n",
      "Epoch 1/6\n",
      "88/88 [==============================] - 28s 279ms/step - loss: 0.8407\n",
      "Epoch 2/6\n",
      "88/88 [==============================] - 25s 280ms/step - loss: 0.6270\n",
      "Epoch 3/6\n",
      "88/88 [==============================] - 25s 285ms/step - loss: 0.5807\n",
      "Epoch 4/6\n",
      "88/88 [==============================] - 27s 306ms/step - loss: 0.5517\n",
      "Epoch 5/6\n",
      "66/88 [=====================>........] - ETA: 6s - loss: 0.5258"
     ]
    }
   ],
   "source": [
    "all_years = df.index.year.unique()\n",
    "rmses = []\n",
    "r2s = []\n",
    "max_resids = []\n",
    "\n",
    "print(\"----- Vanilla RNN With Lags -----\")\n",
    "for i in range(len(all_years)-2):\n",
    "    print(all_years[i], \"-->\", all_years[i+1])\n",
    "    train_data = X_[X_.index.year == all_years[i]]\n",
    "    test_data = X_[X_.index.year == all_years[i+1]]\n",
    "\n",
    "    X_train = train_data.drop('calls', axis=1).values\n",
    "    y_train = train_data[['calls']].values\n",
    "    X_test = test_data.drop('calls', axis=1).values\n",
    "    y_test = test_data[['calls']].values\n",
    "\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_X.fit(X_train)\n",
    "    scaler_y = StandardScaler()\n",
    "    scaler_y.fit(y_train)\n",
    "\n",
    "    X_train_scaled = scaler_X.transform(X_train)\n",
    "    y_train_scaled = scaler_y.transform(y_train)\n",
    "    X_test_scaled = scaler_X.transform(X_test)\n",
    "    y_test_scaled = scaler_y.transform(y_test)\n",
    "\n",
    "    X_train_scaled = np.clip(X_train_scaled, -3, 3)\n",
    "    X_test_scaled = np.clip(X_test_scaled, -3, 3)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(200, activation='relu', return_sequences=True, input_shape=(n_steps, 1)), )\n",
    "    model.add(Dropout(0.8))\n",
    "    model.add(SimpleRNN(units=150, activation='relu', return_sequences=True))\n",
    "    model.add(Dropout(0.6))\n",
    "    model.add(SimpleRNN(units=100, activation='relu', return_sequences=True))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(SimpleRNN(units=50, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    # fit the model to the training data\n",
    "    model.fit(X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1),\n",
    "              y_train_scaled,\n",
    "              batch_size=100,\n",
    "              epochs=6)\n",
    "    # make predictions for test data\n",
    "    predictions = model.predict(X_test.reshape(X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "    test_preds = scaler_y.inverse_transform(predictions)\n",
    "    actual_calls = y_test\n",
    "    print(\"Train\")\n",
    "    evaluate_fit(scaler_y.inverse_transform(model.predict(X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1))),\n",
    "                 y_train)\n",
    "    print(\"Test\")\n",
    "    rmse, r2, max_resid = evaluate_fit(test_preds, actual_calls)\n",
    "    rmses.append(rmse)\n",
    "    r2s.append(r2)\n",
    "    max_resids.append(max_resid)\n",
    "\n",
    "print()\n",
    "print(\"Mean RMSE:\", np.mean(rmses))\n",
    "print(\"Mean r2:\", np.mean(r2s))\n",
    "print(\"Max resid:\", np.mean(max_resids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " 341/2667 [==>...........................] - ETA: 1:09 - loss: 0.6268"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[97], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmse\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[39m# fit the model to the training data\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train_scaled\u001b[39m.\u001b[39;49mreshape(X_train_scaled\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m], X_train_scaled\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m], \u001b[39m1\u001b[39;49m),\n\u001b[0;32m     29\u001b[0m             y_train_scaled,\n\u001b[0;32m     30\u001b[0m             batch_size\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m,\n\u001b[0;32m     31\u001b[0m             epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n\u001b[0;32m     32\u001b[0m \u001b[39m# make predictions for test data\u001b[39;00m\n\u001b[0;32m     33\u001b[0m predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test\u001b[39m.\u001b[39mreshape(X_test\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], X_test\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], \u001b[39m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\peteramerkhanian\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\peteramerkhanian\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1677\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1678\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1679\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1682\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1683\u001b[0m ):\n\u001b[0;32m   1684\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1685\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1687\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\peteramerkhanian\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\peteramerkhanian\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\peteramerkhanian\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    923\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    924\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    925\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 926\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_no_variable_creation_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    928\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    929\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\peteramerkhanian\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m    142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\peteramerkhanian\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\peteramerkhanian\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    382\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    383\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    384\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    385\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    386\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    387\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    389\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    390\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    394\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\peteramerkhanian\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_data = X_.iloc[:80000, :]\n",
    "test_data = X_.iloc[80000:, :]\n",
    "\n",
    "X_train = train_data.drop('calls', axis=1).values\n",
    "y_train = train_data[['calls']].values\n",
    "X_test = test_data.drop('calls', axis=1).values\n",
    "y_test = test_data[['calls']].values\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "scaler_X.fit(X_train)\n",
    "scaler_y = StandardScaler()\n",
    "scaler_y.fit(y_train)\n",
    "\n",
    "X_train_scaled = scaler_X.transform(X_train)\n",
    "y_train_scaled = scaler_y.transform(y_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "y_test_scaled = scaler_y.transform(y_test)\n",
    "\n",
    "X_train_scaled = np.clip(X_train_scaled, -3, 3)\n",
    "X_test_scaled = np.clip(X_test_scaled, -3, 3)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(48, activation='relu', input_shape=(n_steps, 1)), )\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# fit the model to the training data\n",
    "model.fit(X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1),\n",
    "            y_train_scaled,\n",
    "            batch_size=30,\n",
    "            epochs=5)\n",
    "# make predictions for test data\n",
    "predictions = model.predict(X_test.reshape(X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "test_preds = scaler_y.inverse_transform(predictions)\n",
    "actual_calls = y_test\n",
    "print(\"Train\")\n",
    "evaluate_fit(scaler_y.inverse_transform(model.predict(X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1))),\n",
    "                y_train)\n",
    "print(\"Test\")\n",
    "rmse, r2, max_resid = evaluate_fit(test_preds, actual_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "8755    2\n",
       "8756    0\n",
       "8757    0\n",
       "8758    1\n",
       "8759    1\n",
       "Length: 8760, dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(np.ravel(actual_calls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.97131070259196"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(pd.Series(np.ravel(actual_calls)), pd.Series(np.ravel(test_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\peteramerkhanian\\anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py:2845: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  c = cov(x, y, rowvar, dtype=dtype)\n",
      "c:\\Users\\peteramerkhanian\\anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py:2704: RuntimeWarning: divide by zero encountered in divide\n",
      "  c *= np.true_divide(1, fact)\n",
      "c:\\Users\\peteramerkhanian\\anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py:2704: RuntimeWarning: invalid value encountered in multiply\n",
      "  c *= np.true_divide(1, fact)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       ...,\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(actual_calls, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "info251",
   "language": "python",
   "name": "info251"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8ce4c5ed7b9c8ed2521015bed597fb57332d51347537394f12a49727595053f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
